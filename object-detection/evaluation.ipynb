{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/Users/kristian/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/Users/kristian/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [10.0s elapsed, 0s remaining, 527.2 samples/s]      \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from numpyencoder import NumpyEncoder\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import time\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "# Load the COCO-2017 validation split into a FiftyOne dataset\n",
    "#\n",
    "# This will download the dataset from the web, if necessary\n",
    "#\n",
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n",
    "#test17 = foz.load_zoo_dataset(\"coco-2017\", split=\"test\")\n",
    "\n",
    "# Give the dataset a new name, and make it persistent so that you can\n",
    "# work with it in future sessions\n",
    "dataset.name = \"coco-2017-sets\"\n",
    "dataset.persistent = True\n",
    "#test17.name = \"coco-2017-test-example\"\n",
    "#test17.persistent = True\n",
    "# Visualize the in the App\n",
    "#session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v2/frameworks/TensorFlow2/variations/ssd-mobilenet-v2/versions/1\"\n",
    "model = hub.load(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "sampled_dataset = dataset.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_emissions()\n",
    "def inference(sampled_dataset):\n",
    "    coco_results = []\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Doing inference for\", len(sampled_dataset), \"images using the full pre-trained ssd mobilnet v2...\")\n",
    "    start_time = time.time()\n",
    "    for sample in sampled_dataset:\n",
    "        coco_sample_results = []\n",
    "\n",
    "        image_path = sample.filepath\n",
    "        image = tf.image.decode_image(tf.io.read_file(image_path), channels=3).numpy()\n",
    "        image_height, image_width, _ = image.shape\n",
    "        image_id = int(image_path[-16:-4])\n",
    "        input_image = tf.convert_to_tensor(image)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "        input_image = tf.cast(input_image, tf.uint8) \n",
    "\n",
    "        detections = model(input_image)\n",
    "\n",
    "        boxes = detections[\"detection_boxes\"][0].numpy()\n",
    "        scores = detections[\"detection_scores\"][0].numpy()\n",
    "        labels = detections[\"detection_classes\"][0].numpy().astype(int)\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_sample_results.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"bbox\":  [\n",
    "                        box[1] * image_width,\n",
    "                        box[0] * image_height,\n",
    "                        (box[3] - box[1]) * image_width,\n",
    "                        (box[2] - box[0]) * image_height,\n",
    "                    ],\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        coco_results.extend(coco_sample_results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inferencetime = end_time - start_time\n",
    "    print(\"Inference time:\", inferencetime)\n",
    "    print(\"___________________________________________________________\")\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:49:07] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:49:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:49:07] No GPU found.\n",
      "[codecarbon INFO @ 09:49:07] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:49:07] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 09:49:07] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 09:49:07] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 09:49:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:49:07]   Platform system: macOS-13.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 09:49:07]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 09:49:07]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 09:49:07]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 09:49:07]   CPU count: 8\n",
      "[codecarbon INFO @ 09:49:07]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 09:49:07]   GPU count: None\n",
      "[codecarbon INFO @ 09:49:07]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Doing inference for 5000 images using the full pre-trained ssd mobilnet v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:49:25] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:49:25] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:49:25] 0.000202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:49:40] Energy consumed for RAM : 0.000050 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:49:40] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:49:40] 0.000404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:49:55] Energy consumed for RAM : 0.000075 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:49:55] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:49:55] 0.000606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:50:10] Energy consumed for RAM : 0.000100 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:50:10] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:50:10] 0.000809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:50:25] Energy consumed for RAM : 0.000125 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:50:25] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:50:25] 0.001011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:50:40] Energy consumed for RAM : 0.000150 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:50:40] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:50:40] 0.001213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:50:55] Energy consumed for RAM : 0.000175 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:50:55] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:50:55] 0.001415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:51:10] Energy consumed for RAM : 0.000200 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:51:10] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:51:10] 0.001617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:51:25] Energy consumed for RAM : 0.000225 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:51:25] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:51:25] 0.001819 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:51:40] Energy consumed for RAM : 0.000250 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:51:40] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:51:40] 0.002021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:51:55] Energy consumed for RAM : 0.000275 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:51:55] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:51:55] 0.002223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:52:10] Energy consumed for RAM : 0.000300 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:52:10] Energy consumed for all CPUs : 0.002127 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:52:10] 0.002427 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:52:25] Energy consumed for RAM : 0.000325 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:52:25] Energy consumed for all CPUs : 0.002304 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:52:25] 0.002630 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:52:40] Energy consumed for RAM : 0.000350 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:52:40] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:52:40] 0.002832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:52:45] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 09:52:45] Energy consumed for RAM : 0.000359 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:52:45] Energy consumed for all CPUs : 0.002542 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 09:52:45] 0.002901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:52:45] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 215.36327505111694\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "coco_results = inference(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to json file...\n",
      "coco_results.json created & saved!\n",
      "___________________________________________________________\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=2.14s)\n",
      "creating index...\n",
      "index created!\n",
      "___________________________________________________________\n",
      "Evaluating the model...\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=16.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.20s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.204\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.174\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.077\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.657\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving results to json file...\")\n",
    "coco_results_path = \"coco_results.json\"\n",
    "with open(coco_results_path, \"w\") as f:\n",
    "    json.dump(coco_results, f, cls=NumpyEncoder)\n",
    "print(\"coco_results.json created & saved!\")\n",
    "\n",
    "print(\"___________________________________________________________\")\n",
    "# Load ground truth annotations\n",
    "gt_path = 'data/coco/annotations/instances_val2017.json'\n",
    "coco_gt = COCO(gt_path)\n",
    "\n",
    "# Our prediction results\n",
    "coco_dt = coco_gt.loadRes(coco_results_path)\n",
    "print(\"___________________________________________________________\")\n",
    "print(\"Evaluating the model...\")\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfLite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_dataset = dataset.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    input_image = tf.convert_to_tensor(image)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "    input_image = tf.cast(input_image, tf.uint8) \n",
    "    image = tf.image.resize(image, (320, 320)) \n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    return input_image\n",
    "\n",
    "def representative_data_gen():\n",
    "    for sample in representative_dataset:\n",
    "        image_path = sample.filepath        \n",
    "        image = tf.image.decode_image(tf.io.read_file(image_path), channels=3)\n",
    "        image = preprocess_image(image) \n",
    "        yield [image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### int8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpjmgloeay/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpjmgloeay/assets\n",
      "/Users/kristian/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-11-20 09:54:48.135050: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-20 09:54:48.135067: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "model_int8.tflite created & saved!\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import lite\n",
    "\n",
    "# int8 quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "\n",
    "converter.representative_dataset = representative_data_gen \n",
    "\n",
    "tflite_model_int8 = converter.convert()\n",
    "\n",
    "print(\"Saving the model...\")\n",
    "with open(\"model_int8.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_int8)\n",
    "print(\"model_int8.tflite created & saved!\")\n",
    "print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float16 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp_2e5v0s2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp_2e5v0s2/assets\n",
      "2023-11-20 10:00:47.684993: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-20 10:00:47.685007: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 271, Total Ops 2164, % non-converted = 12.52 %\n",
      " * 271 ARITH ops\n",
      "\n",
      "- arith.constant:  271 occurrences  (f16: 160, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 161)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "model_fp16.tflite created & saved!\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_fp16 = converter.convert()\n",
    "print(\"Saving the model...\")\n",
    "with open(\"model_fp16.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_fp16)\n",
    "print(\"model_fp16.tflite created & saved!\")\n",
    "print(\"___________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float32 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpjwxe65gm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpjwxe65gm/assets\n",
      "2023-11-20 10:00:59.238302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-20 10:00:59.238312: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "model_fp32.tflite created & saved!\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float32]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_fp32 = converter.convert()\n",
    "\n",
    "print(\"Saving the model...\")\n",
    "with open(\"model_fp32.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_fp32)\n",
    "print(\"model_fp32.tflite created & saved!\")\n",
    "print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def setup_model(model_content):\n",
    "    print(\"___________________________________________________________\")\n",
    "    print(\"Setting up the model...\")\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_content)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = (1, 320, 320, 3)  \n",
    "    interpreter.resize_tensor_input(input_details[0]['index'], input_shape)\n",
    "    interpreter.resize_tensor_input(output_details[0]['index'], input_shape)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return interpreter, input_details, output_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(image_path, size=320):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    if height > width:\n",
    "        new_height = size\n",
    "        scale = size / height\n",
    "        new_width = int(scale * width)\n",
    "    else:\n",
    "        new_width = size\n",
    "        scale = size / width\n",
    "        new_height = int(scale * height)\n",
    "\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    padded_image = np.zeros((size, size, channels), dtype=np.uint8)\n",
    "    top = (size - new_height) // 2\n",
    "    left = (size - new_width) // 2\n",
    "    padded_image[top:top + new_height, left:left + new_width] = resized_image\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(padded_image)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    return input_tensor, new_height, new_width, height, width\n",
    "\n",
    "def rescale_box(image_id, detection_boxes, detection_classes, detection_scores, new_height, new_width, height, width, size=320):\n",
    "    all_results = []\n",
    "\n",
    "    box = np.round(detection_boxes[0] * size).astype(int)\n",
    "\n",
    "    height_scale = height / new_height\n",
    "    width_scale = width / new_width\n",
    "\n",
    "    top_padding = (size - new_height) // 2\n",
    "    left_padding = (size - new_width) // 2\n",
    "\n",
    "    for i in range(len(detection_boxes[0])):\n",
    "        ystart, xstart, yend, xend = box[i]\n",
    "\n",
    "        ystart = (ystart - top_padding) * height_scale\n",
    "        xstart = (xstart - left_padding) * width_scale\n",
    "        yend = (yend - top_padding) * height_scale\n",
    "        xend = (xend - left_padding) * width_scale\n",
    "\n",
    "        box_width = xend - xstart\n",
    "        box_height = yend - ystart\n",
    "\n",
    "        result_image = {\n",
    "            \"image_id\": int(image_id),\n",
    "            \"category_id\": int(detection_classes[0][i]),\n",
    "            \"bbox\": [xstart, ystart, box_width, box_height],\n",
    "            \"score\": float(detection_scores[0][i])\n",
    "        }\n",
    "\n",
    "        all_results.append(result_image)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "@track_emissions()\n",
    "def inference(sampled_dataset, interpreter, input_details, output_details):\n",
    "    coco_results = []\n",
    "\n",
    "    for sample in sampled_dataset:\n",
    "        image_path = sample.filepath\n",
    "        image_id = int(image_path[-16:-4])\n",
    "\n",
    "        input_image, new_height, new_width, height, width = scale_image(image_path)\n",
    "        boxes, classes, scores = test_model(input_image, interpreter, input_details, output_details)\n",
    "        predictions = rescale_box(image_id, boxes, classes, scores, new_height, new_width, height, width)\n",
    "\n",
    "        coco_results.extend(predictions)\n",
    "\n",
    "    return coco_results\n",
    "\n",
    "def test_model(input_tensor, interpreter, input_details, output_details):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    detection_boxes = interpreter.get_tensor(output_details[4]['index'])\n",
    "    detection_classes = interpreter.get_tensor(output_details[5]['index'])\n",
    "    detection_scores = interpreter.get_tensor(output_details[6]['index'])\n",
    "\n",
    "    return detection_boxes, detection_classes, detection_scores\n",
    "\n",
    "\n",
    "def evaluate(coco_results):\n",
    "    coco_results_path = \"coco_results.json\"\n",
    "    with open(coco_results_path, \"w\") as f:\n",
    "        json.dump(coco_results, f, cls=NumpyEncoder)\n",
    "\n",
    "    gt_path = 'data/coco/annotations/instances_val2017.json'\n",
    "    coco_gt = COCO(gt_path)\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(coco_results_path)\n",
    "\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"model_int8.tflite\", \"model_fp16.tflite\", \"model_fp32.tflite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:20:05] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:20:05] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:20:05] No GPU found.\n",
      "[codecarbon INFO @ 10:20:05] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:20:05] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:20:05] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:20:05] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 10:20:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:20:05]   Platform system: macOS-13.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 10:20:05]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 10:20:05]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 10:20:05]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 10:20:05]   CPU count: 8\n",
      "[codecarbon INFO @ 10:20:05]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 10:20:05]   GPU count: None\n",
      "[codecarbon INFO @ 10:20:05]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model:  model_int8.tflite\n",
      "___________________________________________________________\n",
      "Setting up the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:20:20] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:20:20] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:20:20] 0.000202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:20:35] Energy consumed for RAM : 0.000050 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:20:35] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:20:35] 0.000404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:20:50] Energy consumed for RAM : 0.000075 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:20:50] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:20:50] 0.000606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:21:05] Energy consumed for RAM : 0.000100 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:21:05] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:21:05] 0.000809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:21:20] Energy consumed for RAM : 0.000125 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:21:20] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:21:20] 0.001011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:21:35] Energy consumed for RAM : 0.000150 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:21:35] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:21:35] 0.001213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:21:50] Energy consumed for RAM : 0.000175 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:21:50] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:21:50] 0.001415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:22:05] Energy consumed for RAM : 0.000200 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:22:05] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:22:05] 0.001617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:22:20] Energy consumed for RAM : 0.000225 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:22:20] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:22:20] 0.001819 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:22:35] Energy consumed for RAM : 0.000250 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:22:35] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:22:35] 0.002021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:22:50] Energy consumed for RAM : 0.000275 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:22:50] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:22:50] 0.002224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:23:05] Energy consumed for RAM : 0.000300 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:23:05] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:23:05] 0.002426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:23:20] Energy consumed for RAM : 0.000325 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:23:20] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:23:20] 0.002628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:23:30] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:23:30] Energy consumed for RAM : 0.000341 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:23:30] Energy consumed for all CPUs : 0.002414 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:23:30] 0.002755 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:23:30] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time:  205.02662086486816\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.71s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=14.53s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.27s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.136\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.168\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.102\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.161\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.169\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.162\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:23:54] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:23:54] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:23:54] No GPU found.\n",
      "[codecarbon INFO @ 10:23:54] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:23:54] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:23:54] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:23:54] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 10:23:54] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:23:54]   Platform system: macOS-13.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 10:23:54]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 10:23:54]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 10:23:54]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 10:23:54]   CPU count: 8\n",
      "[codecarbon INFO @ 10:23:54]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 10:23:54]   GPU count: None\n",
      "[codecarbon INFO @ 10:23:54]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________\n",
      "Running model:  model_fp16.tflite\n",
      "___________________________________________________________\n",
      "Setting up the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:24:12] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:24:12] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:24:12] 0.000202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:24:27] Energy consumed for RAM : 0.000050 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:24:27] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:24:27] 0.000404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:24:42] Energy consumed for RAM : 0.000075 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:24:42] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:24:42] 0.000606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:24:57] Energy consumed for RAM : 0.000100 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:24:57] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:24:57] 0.000809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:25:12] Energy consumed for RAM : 0.000125 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:25:12] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:25:12] 0.001011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:25:27] Energy consumed for RAM : 0.000150 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:25:27] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:25:27] 0.001213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:25:42] Energy consumed for RAM : 0.000175 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:25:42] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:25:42] 0.001415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:25:57] Energy consumed for RAM : 0.000200 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:25:57] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:25:57] 0.001617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:26:12] Energy consumed for RAM : 0.000225 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:26:12] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:26:12] 0.001819 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:26:27] Energy consumed for RAM : 0.000250 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:26:27] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:26:27] 0.002022 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:26:42] Energy consumed for RAM : 0.000275 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:26:42] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:26:42] 0.002224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:26:57] Energy consumed for RAM : 0.000300 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:26:57] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:26:57] 0.002426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:27:01] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:27:01] Energy consumed for RAM : 0.000306 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:27:01] Energy consumed for all CPUs : 0.002170 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:27:01] 0.002476 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:27:01] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time:  187.19544315338135\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.79s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.67s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.31s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.149\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.258\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.268\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:27:26] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:27:26] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:27:26] No GPU found.\n",
      "[codecarbon INFO @ 10:27:26] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:27:26] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:27:26] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:27:26] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 10:27:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:27:26]   Platform system: macOS-13.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 10:27:26]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 10:27:26]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 10:27:26]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 10:27:26]   CPU count: 8\n",
      "[codecarbon INFO @ 10:27:26]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 10:27:26]   GPU count: None\n",
      "[codecarbon INFO @ 10:27:26]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________\n",
      "Running model:  model_fp32.tflite\n",
      "___________________________________________________________\n",
      "Setting up the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:27:42] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:27:42] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:27:42] 0.000202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:27:57] Energy consumed for RAM : 0.000050 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:27:57] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:27:57] 0.000404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:28:12] Energy consumed for RAM : 0.000075 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:28:12] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:28:12] 0.000606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:28:27] Energy consumed for RAM : 0.000100 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:28:27] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:28:27] 0.000809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:28:42] Energy consumed for RAM : 0.000125 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:28:42] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:28:42] 0.001011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:28:57] Energy consumed for RAM : 0.000150 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:28:57] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:28:57] 0.001213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:29:12] Energy consumed for RAM : 0.000175 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:29:12] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:29:12] 0.001415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:29:27] Energy consumed for RAM : 0.000200 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:29:27] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:29:27] 0.001617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:29:42] Energy consumed for RAM : 0.000225 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:29:42] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:29:42] 0.001819 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:29:57] Energy consumed for RAM : 0.000250 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:29:57] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:29:57] 0.002021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:30:12] Energy consumed for RAM : 0.000275 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:30:12] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:30:12] 0.002223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:30:27] Energy consumed for RAM : 0.000300 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:30:27] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:30:27] 0.002426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:30:30] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:30:30] Energy consumed for RAM : 0.000305 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:30:30] Energy consumed for all CPUs : 0.002163 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:30:30] 0.002468 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:30:30] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time:  184.5064160823822\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.76s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.85s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.35s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.149\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.258\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.335\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.257\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.582\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"Running model: \", model)\n",
    "    interpreter, input_details, output_details = setup_model(model)\n",
    "    start_time = time.time()\n",
    "    coco_results = inference(sampled_dataset, interpreter, input_details, output_details)\n",
    "    end_time = time.time()\n",
    "    print(\"Inference time: \", end_time - start_time)\n",
    "    evaluation = evaluate(coco_results)\n",
    "    print(\"______________________________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
