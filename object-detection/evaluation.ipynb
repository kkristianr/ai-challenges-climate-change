{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activitynet-100', 'activitynet-200', 'bdd100k', 'caltech101', 'caltech256', 'cifar10', 'cifar100', 'cityscapes', 'coco-2014', 'coco-2017', 'fashion-mnist', 'fiw', 'hmdb51', 'imagenet-2012', 'imagenet-sample', 'kinetics-400', 'kinetics-600', 'kinetics-700', 'kinetics-700-2020', 'kitti', 'kitti-multiview', 'lfw', 'mnist', 'open-images-v6', 'open-images-v7', 'quickstart', 'quickstart-geo', 'quickstart-groups', 'quickstart-video', 'sama-coco', 'ucf101', 'voc-2007', 'voc-2012']\n",
      "Downloading split 'validation' to '/Users/kristian/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/Users/kristian/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found annotations at '/Users/kristian/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.coco:Found annotations at '/Users/kristian/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.coco:Images already downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing download of split 'validation' is sufficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone.utils.eval.detection import evaluate_detections, DetectionResults\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from numpyencoder import NumpyEncoder\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import time\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "\n",
    "\n",
    "# List available zoo datasets\n",
    "print(foz.list_zoo_datasets())\n",
    "\n",
    "#\n",
    "# Load the COCO-2017 validation split into a FiftyOne dataset\n",
    "#\n",
    "# This will download the dataset from the web, if necessary\n",
    "#\n",
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n",
    "#test17 = foz.load_zoo_dataset(\"coco-2017\", split=\"test\")\n",
    "\n",
    "# Give the dataset a new name, and make it persistent so that you can\n",
    "# work with it in future sessions\n",
    "dataset.name = \"coco-2017-valix\"\n",
    "dataset.persistent = True\n",
    "#test17.name = \"coco-2017-test-example\"\n",
    "#test17.persistent = True\n",
    "# Visualize the in the App\n",
    "#session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v2/frameworks/TensorFlow2/variations/ssd-mobilenet-v2/versions/1\"\n",
    "model = hub.load(model_url)\n",
    "\n",
    "#TO DO: Get size of model ssd mobilenet v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    input_image = tf.convert_to_tensor(image)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "    input_image = tf.cast(input_image, tf.uint8) \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_infer = 5000\n",
    "sampled_dataset = dataset.take(num_images_to_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_results = []\n",
    "\n",
    "# Iterate through each sample in the dataset\n",
    "for sample in sampled_dataset:\n",
    "    coco_sample_results = []\n",
    "\n",
    "    # Load the image\n",
    "    image_path = sample.filepath\n",
    "    image = tf.image.decode_image(tf.io.read_file(image_path), channels=3).numpy()\n",
    "    image_height, image_width, _ = image.shape\n",
    "    image_id = int(image_path[-16:-4])\n",
    "\n",
    "    # Perform inference\n",
    "    input_image = preprocess_image(image)\n",
    "    detections = model(input_image)\n",
    "\n",
    "    # Extract relevant information from the detections\n",
    "    boxes = detections[\"detection_boxes\"][0].numpy()\n",
    "    scores = detections[\"detection_scores\"][0].numpy()\n",
    "    labels = detections[\"detection_classes\"][0].numpy().astype(int)\n",
    "\n",
    "    # Append image id to results\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Convert to COCO format\n",
    "        coco_sample_results.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": label,\n",
    "                # Scale box to image size\n",
    "                \"bbox\":  [\n",
    "                    box[1] * image_width,\n",
    "                    box[0] * image_height,\n",
    "                    (box[3] - box[1]) * image_width,\n",
    "                    (box[2] - box[0]) * image_height,\n",
    "                ],\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    coco_results.extend(coco_sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=2.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=17.14s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.14s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.204\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.174\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.077\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.657\n"
     ]
    }
   ],
   "source": [
    "# Save the results to disk\n",
    "coco_results_path = \"coco_results.json\"\n",
    "with open(coco_results_path, \"w\") as f:\n",
    "    json.dump(coco_results, f, cls=NumpyEncoder)\n",
    "\n",
    "# Load the COCO annotations for the validation split\n",
    "gt_path = 'data/coco/annotations/instances_val2017.json'\n",
    "coco_gt = COCO(gt_path)\n",
    "\n",
    "# Load the COCO results\n",
    "coco_dt = coco_gt.loadRes(coco_results_path)\n",
    "\n",
    "# Evaluate the results\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "#coco_eval.params.imgIds  = imgIds\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfLite\n",
    "#### float32 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_dataset = dataset.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp8dz9mbxe/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp8dz9mbxe/assets\n",
      "/Users/kristian/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-11-18 16:46:19.301035: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-18 16:46:19.301054: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import lite\n",
    "\n",
    "# Convert the model to TensorFlow Lite format with float32 quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "    for sample in representative_dataset:\n",
    "        # Preprocess the image\n",
    "        image_path = sample.filepath\n",
    "        image = tf.image.decode_image(tf.io.read_file(image_path), channels=3)\n",
    "        image = preprocess_image(image)  \n",
    "\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen \n",
    "\n",
    "tflite_model_int8 = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open(\"model_int8.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_int8)\n",
    "\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "desired_size = 320\n",
    "\n",
    "new_input_shape = (1, desired_size, desired_size, 3)  # Neue Eingabegröße: (Batch, Höhe, Breite, Kanäle)\n",
    "interpreter.resize_tensor_input(input_details[0]['index'], new_input_shape)\n",
    "interpreter.resize_tensor_input(output_details[0]['index'], new_input_shape)\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "def test_model(input_tensor):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    detection_boxes = interpreter.get_tensor(output_details[4]['index'])\n",
    "    detection_classes = interpreter.get_tensor(output_details[5]['index'])\n",
    "    detection_scores = interpreter.get_tensor(output_details[6]['index'])\n",
    "\n",
    "    return detection_boxes, detection_classes, detection_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare box sizes for evaluation\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    if height > width:\n",
    "        new_height = desired_size\n",
    "        scale = desired_size / height\n",
    "        new_width = int(scale * width)\n",
    "    else:\n",
    "        new_width = desired_size\n",
    "        scale = desired_size / width\n",
    "        new_height = int(scale * height)\n",
    "\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    padded_image = np.zeros((desired_size, desired_size, channels), dtype=np.uint8)\n",
    "    top = (desired_size - new_height) // 2\n",
    "    left = (desired_size - new_width) // 2\n",
    "    padded_image[top:top + new_height, left:left + new_width] = resized_image\n",
    "    padded_image = padded_image[..., ::-1]  # BGR to RGB\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(padded_image)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    return padded_image, input_tensor, new_height, new_width, height, width\n",
    "\n",
    "def prepare_results(image_id, detection_boxes, detection_classes, detection_scores, new_height, new_width, height, width):\n",
    "    prepared_results = []\n",
    "\n",
    "    pbox = np.round(detection_boxes[0] * desired_size).astype(int)\n",
    "\n",
    "    height_scale = height / new_height\n",
    "    width_scale = width / new_width\n",
    "\n",
    "    top_padding = (desired_size - new_height) // 2\n",
    "    left_padding = (desired_size - new_width) // 2\n",
    "\n",
    "    for i in range(len(detection_boxes[0])):\n",
    "        # Umrechnen der bbox-Koordinaten von Prozent in Pixel unter Berücksichtigung der Skalierung und des Paddings\n",
    "        ystart, xstart, yend, xend = pbox[i]\n",
    "\n",
    "        ystart = (ystart - top_padding) * height_scale\n",
    "        xstart = (xstart - left_padding) * width_scale\n",
    "        yend = (yend - top_padding) * height_scale\n",
    "        xend = (xend - left_padding) * width_scale\n",
    "\n",
    "        box_width = xend - xstart\n",
    "        box_height = yend - ystart\n",
    "\n",
    "        result_entry = {\n",
    "            \"image_id\": int(image_id),\n",
    "            \"category_id\": int(detection_classes[0][i]),\n",
    "            \"bbox\": [xstart, ystart, box_width, box_height],\n",
    "            \"score\": float(detection_scores[0][i])\n",
    "        }\n",
    "\n",
    "        prepared_results.append(result_entry)\n",
    "\n",
    "    return prepared_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do inference with the new model for the sampled dataset\n",
    "all_predictions = []\n",
    "\n",
    "for sample in sampled_dataset:\n",
    "    # Load and preprocess the image\n",
    "    image_path = sample.filepath\n",
    "    image_id = int(image_path[-16:-4])\n",
    "\n",
    "    image, input_image, new_height, new_width, height, width = load_image(image_path)\n",
    "    boxes, classes, scores = test_model(input_image)\n",
    "    predictions = prepare_results(image_id, boxes, classes, scores, new_height, new_width, height, width)\n",
    "\n",
    "    all_predictions.extend(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #Do inference with the new model for the sampled dataset\\nall_predictions = []\\n\\nfor sample in sampled_dataset:\\n    # Load and preprocess the image\\n    image_path = sample.filepath\\n    image = cv2.imread(image_path)\\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n    image_id = int(image_path[-16:-4])\\n    image_height, image_width, _ = image.shape\\n\\n\\n    # Resize the image to match the expected dimensions\\n    height, width = 320, 320  # Adjust these values based on your model\\'s input shape\\n    input_image = cv2.resize(image, (width, height))\\n    \\n    # Normalize and expand dimensions\\n    input_image = input_image / 255.0  # Normalize to [0, 1]\\n    input_image = np.expand_dims(input_image, axis=0).astype(np.uint8)\\n\\n    # Run inference\\n    boxes, classes, scores = test_model(input_image)\\n\\n    # Process the results as needed\\n    # (e.g., visualize the detected objects on the image, print results, etc.)\\n    predictions = []\\n\\n    for box, score, label in zip(boxes[0], scores[0], classes[0]):\\n        predictions.append({\\n            \"image_id\": image_id,\\n            \"category_id\": int(label),\\n            \"bbox\":  [\\n                    box[1] * image_width,\\n                    box[0] * image_height,\\n                    (box[3] - box[1]) * image_width,\\n                    (box[2] - box[0]) * image_height,\\n                ],\\n            \"score\": float(score),\\n        })\\n\\n    all_predictions.extend(predictions) '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #Do inference with the new model for the sampled dataset\n",
    "all_predictions = []\n",
    "\n",
    "for sample in sampled_dataset:\n",
    "    # Load and preprocess the image\n",
    "    image_path = sample.filepath\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_id = int(image_path[-16:-4])\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "\n",
    "    # Resize the image to match the expected dimensions\n",
    "    height, width = 320, 320  # Adjust these values based on your model's input shape\n",
    "    input_image = cv2.resize(image, (width, height))\n",
    "    \n",
    "    # Normalize and expand dimensions\n",
    "    input_image = input_image / 255.0  # Normalize to [0, 1]\n",
    "    input_image = np.expand_dims(input_image, axis=0).astype(np.uint8)\n",
    "\n",
    "    # Run inference\n",
    "    boxes, classes, scores = test_model(input_image)\n",
    "\n",
    "    # Process the results as needed\n",
    "    # (e.g., visualize the detected objects on the image, print results, etc.)\n",
    "    predictions = []\n",
    "\n",
    "    for box, score, label in zip(boxes[0], scores[0], classes[0]):\n",
    "        predictions.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": int(label),\n",
    "            \"bbox\":  [\n",
    "                    box[1] * image_width,\n",
    "                    box[0] * image_height,\n",
    "                    (box[3] - box[1]) * image_width,\n",
    "                    (box[2] - box[0]) * image_height,\n",
    "                ],\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "\n",
    "    all_predictions.extend(predictions) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=2.63s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.11s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.161\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.076\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.178\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.189\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.026\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n"
     ]
    }
   ],
   "source": [
    "#Dump json\n",
    "coco_results_path = \"coco_results_int8.json\"\n",
    "with open(coco_results_path, \"w\") as f:\n",
    "    json.dump(all_predictions, f, cls=NumpyEncoder)\n",
    "\n",
    "coco_results = coco_gt.loadRes(coco_results_path)\n",
    "coco_eval = COCOeval(coco_gt, coco_results, 'bbox')  # 'bbox' indicates bounding box evaluation\n",
    "\n",
    "\n",
    "# Run COCOeval\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
