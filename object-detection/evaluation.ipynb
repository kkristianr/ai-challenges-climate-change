{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/Users/kristian/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/Users/kristian/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found annotations at '/Users/kristian/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.coco:Found annotations at '/Users/kristian/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.coco:Images already downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing download of split 'validation' is sufficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone.utils.eval.detection import evaluate_detections, DetectionResults\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from numpyencoder import NumpyEncoder\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import time\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "# Load the COCO-2017 validation split into a FiftyOne dataset\n",
    "#\n",
    "# This will download the dataset from the web, if necessary\n",
    "#\n",
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n",
    "#test17 = foz.load_zoo_dataset(\"coco-2017\", split=\"test\")\n",
    "\n",
    "# Give the dataset a new name, and make it persistent so that you can\n",
    "# work with it in future sessions\n",
    "dataset.name = \"coco-2017-set\"\n",
    "dataset.persistent = True\n",
    "#test17.name = \"coco-2017-test-example\"\n",
    "#test17.persistent = True\n",
    "# Visualize the in the App\n",
    "#session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v2/frameworks/TensorFlow2/variations/ssd-mobilenet-v2/versions/1\"\n",
    "model = hub.load(model_url)\n",
    "\n",
    "#TO DO: Get size of model ssd mobilenet v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_infer = 3500\n",
    "sampled_dataset = dataset.take(num_images_to_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    input_image = tf.convert_to_tensor(image)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "    input_image = tf.cast(input_image, tf.uint8) \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference on the samples...\n",
      "Inference time for (full) model:  136.57784295082092\n",
      "Inference time per image:  0.03902224084309169\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "coco_results = []\n",
    "\n",
    "# Iterate through each sample in the dataset\n",
    "print(\"Performing inference on the samples...\")\n",
    "start_time = time.time()\n",
    "for sample in sampled_dataset:\n",
    "    coco_sample_results = []\n",
    "\n",
    "    # Load the image\n",
    "    image_path = sample.filepath\n",
    "    image = tf.image.decode_image(tf.io.read_file(image_path), channels=3).numpy()\n",
    "    image_height, image_width, _ = image.shape\n",
    "    image_id = int(image_path[-16:-4])\n",
    "\n",
    "    # Perform inference\n",
    "    input_image = preprocess_image(image)\n",
    "    detections = model(input_image)\n",
    "\n",
    "    # Extract relevant information from the detections\n",
    "    boxes = detections[\"detection_boxes\"][0].numpy()\n",
    "    scores = detections[\"detection_scores\"][0].numpy()\n",
    "    labels = detections[\"detection_classes\"][0].numpy().astype(int)\n",
    "\n",
    "    # Append image id to results\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Convert to COCO format\n",
    "        coco_sample_results.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": label,\n",
    "                # Scale box to image size\n",
    "                \"bbox\":  [\n",
    "                    box[1] * image_width,\n",
    "                    box[0] * image_height,\n",
    "                    (box[3] - box[1]) * image_width,\n",
    "                    (box[2] - box[0]) * image_height,\n",
    "                ],\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    coco_results.extend(coco_sample_results)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Inference time for (full) model: \", end_time - start_time)\n",
    "print(\"Inference time per image: \", (end_time - start_time)/num_images_to_infer)\n",
    "\n",
    "print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.69s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=12.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.243\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.289\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.453\n"
     ]
    }
   ],
   "source": [
    "# Save the results to disk\n",
    "coco_results_path = \"coco_results.json\"\n",
    "with open(coco_results_path, \"w\") as f:\n",
    "    json.dump(coco_results, f, cls=NumpyEncoder)\n",
    "\n",
    "# Load the COCO annotations for the validation split\n",
    "gt_path = 'data/coco/annotations/instances_val2017.json'\n",
    "coco_gt = COCO(gt_path)\n",
    "\n",
    "# Load the COCO results\n",
    "coco_dt = coco_gt.loadRes(coco_results_path)\n",
    "\n",
    "# Evaluate the results\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "#coco_eval.params.imgIds  = imgIds\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfLite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_dataset = dataset.take(250)\n",
    "\n",
    "def representative_data_gen():\n",
    "    for sample in representative_dataset:\n",
    "        # Preprocess the image\n",
    "        image_path = sample.filepath        \n",
    "        image = tf.image.decode_image(tf.io.read_file(image_path), channels=3)\n",
    "        image = preprocess_image(image) \n",
    "        image = tf.image.resize(image, (320, 320)) \n",
    "        image = tf.cast(image, tf.uint8)\n",
    "\n",
    "        yield [image]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### int8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpa34e5qtl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpa34e5qtl/assets\n",
      "/Users/kristian/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-11-18 23:59:19.609419: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-18 23:59:19.609431: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import lite\n",
    "\n",
    "# Convert the model to TensorFlow Lite format with int8 quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "\n",
    "converter.representative_dataset = representative_data_gen \n",
    "\n",
    "tflite_model_int8 = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open(\"model_int8.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float16 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpqbl4qrxj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmpqbl4qrxj/assets\n",
      "2023-11-19 00:15:37.518849: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-19 00:15:37.518877: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 271, Total Ops 2164, % non-converted = 12.52 %\n",
      " * 271 ARITH ops\n",
      "\n",
      "- arith.constant:  271 occurrences  (f16: 160, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 161)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_fp16 = converter.convert()\n",
    "\n",
    "with open(\"model_fp16.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_fp16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float32 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp_rvykcb3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/5_c7kn_13jsgmgqz56r3yx440000gp/T/tmp_rvykcb3/assets\n",
      "2023-11-19 00:15:17.085662: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-19 00:15:17.085676: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 272, Total Ops 2004, % non-converted = 13.57 %\n",
      " * 272 ARITH ops\n",
      "\n",
      "- arith.constant:  272 occurrences  (f32: 161, i32: 111)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 15)\n",
      "  (f32: 3, i1: 1, i32: 3)\n",
      "  (f32: 98, i32: 90)\n",
      "  (f32: 55)\n",
      "  (f32: 17)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 90)\n",
      "  (f32: 295)\n",
      "  (i1: 7)\n",
      "  (i1: 1)\n",
      "  (i1: 90)\n",
      "  (f32: 1)\n",
      "  (f32: 4, i32: 1)\n",
      "  (f32: 14)\n",
      "  (i32: 90)\n",
      "  (f32: 6, i32: 9)\n",
      "  (f32: 5)\n",
      "  (i32: 2)\n",
      "  (f32: 4)\n",
      "  (i64: 1, f32: 106, i1: 1, i32: 98)\n",
      "  (f32: 1)\n",
      "  (f32: 91, i32: 6)\n",
      "  (i32: 103)\n",
      "  (f32: 96)\n",
      "  (f32: 4)\n",
      "  (i32: 105)\n",
      "  (f32: 8, i32: 102)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 6)\n",
      "  (i64: 1)\n"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float32]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_fp32 = converter.convert()\n",
    "\n",
    "with open(\"model_fp32.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def setup_model(model_content):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_content)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    desired_size = 320\n",
    "\n",
    "    new_input_shape = (1, desired_size, desired_size, 3)  \n",
    "    interpreter.resize_tensor_input(input_details[0]['index'], new_input_shape)\n",
    "    interpreter.resize_tensor_input(output_details[0]['index'], new_input_shape)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return interpreter, input_details, output_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare box sizes for evaluation\n",
    "def load_image(image_path, desired_size=320):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    if height > width:\n",
    "        new_height = desired_size\n",
    "        scale = desired_size / height\n",
    "        new_width = int(scale * width)\n",
    "    else:\n",
    "        new_width = desired_size\n",
    "        scale = desired_size / width\n",
    "        new_height = int(scale * height)\n",
    "\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    padded_image = np.zeros((desired_size, desired_size, channels), dtype=np.uint8)\n",
    "    top = (desired_size - new_height) // 2\n",
    "    left = (desired_size - new_width) // 2\n",
    "    padded_image[top:top + new_height, left:left + new_width] = resized_image\n",
    "    padded_image = padded_image[..., ::-1]  # BGR to RGB\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(padded_image)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    return padded_image, input_tensor, new_height, new_width, height, width\n",
    "\n",
    "def prepare_results(image_id, detection_boxes, detection_classes, detection_scores, new_height, new_width, height, width, desired_size=320):\n",
    "    prepared_results = []\n",
    "\n",
    "    pbox = np.round(detection_boxes[0] * desired_size).astype(int)\n",
    "\n",
    "    height_scale = height / new_height\n",
    "    width_scale = width / new_width\n",
    "\n",
    "    top_padding = (desired_size - new_height) // 2\n",
    "    left_padding = (desired_size - new_width) // 2\n",
    "\n",
    "    for i in range(len(detection_boxes[0])):\n",
    "        # Umrechnen der bbox-Koordinaten von Prozent in Pixel unter Ber√ºcksichtigung der Skalierung und des Paddings\n",
    "        ystart, xstart, yend, xend = pbox[i]\n",
    "\n",
    "        ystart = (ystart - top_padding) * height_scale\n",
    "        xstart = (xstart - left_padding) * width_scale\n",
    "        yend = (yend - top_padding) * height_scale\n",
    "        xend = (xend - left_padding) * width_scale\n",
    "\n",
    "        box_width = xend - xstart\n",
    "        box_height = yend - ystart\n",
    "\n",
    "        result_entry = {\n",
    "            \"image_id\": int(image_id),\n",
    "            \"category_id\": int(detection_classes[0][i]),\n",
    "            \"bbox\": [xstart, ystart, box_width, box_height],\n",
    "            \"score\": float(detection_scores[0][i])\n",
    "        }\n",
    "\n",
    "        prepared_results.append(result_entry)\n",
    "\n",
    "    return prepared_results\n",
    "\n",
    "\n",
    "def inference(sampled_dataset, interpreter, input_details, output_details, num_images_to_infer=3500):\n",
    "    sampled_dataset = dataset.take(num_images_to_infer)\n",
    "    coco_results = []\n",
    "\n",
    "    for sample in sampled_dataset:\n",
    "        image_path = sample.filepath\n",
    "        image_id = int(image_path[-16:-4])\n",
    "\n",
    "        image, input_image, new_height, new_width, height, width = load_image(image_path)\n",
    "        boxes, classes, scores = test_model(input_image, interpreter, input_details, output_details)\n",
    "        predictions = prepare_results(image_id, boxes, classes, scores, new_height, new_width, height, width)\n",
    "\n",
    "        coco_results.extend(predictions)\n",
    "\n",
    "    return coco_results\n",
    "\n",
    "def test_model(input_tensor, interpreter, input_details, output_details):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    detection_boxes = interpreter.get_tensor(output_details[4]['index'])\n",
    "    detection_classes = interpreter.get_tensor(output_details[5]['index'])\n",
    "    detection_scores = interpreter.get_tensor(output_details[6]['index'])\n",
    "\n",
    "    return detection_boxes, detection_classes, detection_scores\n",
    "\n",
    "\n",
    "def evaluate(coco_results):\n",
    "    coco_results_path = \"coco_results.json\"\n",
    "    with open(coco_results_path, \"w\") as f:\n",
    "        json.dump(coco_results, f, cls=NumpyEncoder)\n",
    "\n",
    "    # Load the COCO annotations for the validation split\n",
    "    gt_path = 'data/coco/annotations/instances_val2017.json'\n",
    "    coco_gt = COCO(gt_path)\n",
    "\n",
    "    # Load the COCO results\n",
    "    coco_dt = coco_gt.loadRes(coco_results_path)\n",
    "\n",
    "    # Evaluate the results\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    #coco_eval.params.imgIds  = imgIds\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"model_int8.tflite\", \"model_fp16.tflite\", \"model_fp32.tflite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model:  model_int8.tflite\n",
      "Inference time:  144.18134713172913\n",
      "Evaluating model:  model_int8.tflite\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=2.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=10.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.138\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.079\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.126\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.133\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.134\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.280\n",
      "______________________________________________________\n",
      "Running model:  model_fp16.tflite\n",
      "Inference time:  132.36495900154114\n",
      "Evaluating model:  model_fp16.tflite\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=13.47s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.217\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.274\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.136\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.439\n",
      "______________________________________________________\n",
      "Running model:  model_fp32.tflite\n",
      "Inference time:  129.77553009986877\n",
      "Evaluating model:  model_fp32.tflite\n",
      "loading annotations into memory...\n",
      "Done (t=1.76s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.54s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=12.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.13s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.216\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.095\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.445\n",
      "______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"Running model: \", model)\n",
    "    interpreter, input_details, output_details = setup_model(model)\n",
    "    start_time = time.time()\n",
    "    coco_results = inference(sampled_dataset, interpreter, input_details, output_details)\n",
    "    end_time = time.time()\n",
    "    print(\"Inference time: \", end_time - start_time)\n",
    "    print(\"Evaluating model: \", model)\n",
    "    evaluation = evaluate(coco_results)\n",
    "    print(\"______________________________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
